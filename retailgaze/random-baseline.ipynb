{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "from PIL import Image, ImageOps\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import operator\n",
    "import itertools\n",
    "from scipy.io import  loadmat\n",
    "import logging\n",
    "from scipy import signal\n",
    "\n",
    "from utils import data_transforms\n",
    "from utils import get_paste_kernel, kernel_map\n",
    "from utils_logging import setup_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Choose between Recasens or GazeNet\n",
    "\n",
    "- Idea is you can just swap \n",
    "models.recasens, dataloader.recasens, training.train_recasens, etc...\n",
    "- with the following\n",
    "models.gazenet, dataloader.gazenet, training.train_gazenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.gazenet import GazeNet\n",
    "from models.__init__ import save_checkpoint, resume_checkpoint\n",
    "from dataloader.gazenet import GooDataset, GazeDataset\n",
    "from training.train_gazenet import train, test, GazeOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger will save the training and test errors to a .log file \n",
    "logger = setup_logger(name='first_logger', \n",
    "                      log_dir ='./logs/',\n",
    "                      log_file='random_baseline.log',\n",
    "                      log_format = '%(asctime)s %(levelname)s %(message)s',\n",
    "                      verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dataloaders\n",
    "- Choose between GazeDataset (Gazefollow dataset) or GooDataset (GooSynth/GooReal)\n",
    "- Set paths to image directories and pickle paths. For Gazefollow, images_dir and test_images_dir should be the same and both lead to the path containing the train and test folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders for GazeFollow\n",
    "batch_size=32\n",
    "workers=12\n",
    "testbatchsize=16\n",
    "\n",
    "images_dir = '/home/eee198/Documents/datasets/GazeFollowData/'\n",
    "pickle_path = '/home/eee198/Documents/datasets/GazeFollowData/train_annotations.mat'\n",
    "test_images_dir = '/home/eee198/Documents/datasets/GazeFollowData/'\n",
    "test_pickle_path = '/home/eee198/Documents/datasets/GazeFollowData/test_annotations.mat'\n",
    "\n",
    "train_set = GazeDataset(images_dir, pickle_path, 'train')\n",
    "train_data_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "\n",
    "val_set = GazeDataset(test_images_dir, test_pickle_path, 'test')\n",
    "test_data_loader = torch.utils.data.DataLoader(val_set, batch_size=testbatchsize, num_workers=workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Number of Images: 172800\n",
      "==> Number of Images: 19200\n"
     ]
    }
   ],
   "source": [
    "# Dataloaders for GOO\n",
    "batch_size=32\n",
    "workers=12\n",
    "testbatchsize=32\n",
    "\n",
    "images_dir = '/hdd/HENRI/goosynth/1person/GazeDatasets/'\n",
    "pickle_path = '/hdd/HENRI/goosynth/picklefiles/trainpickle2to19human.pickle'\n",
    "test_images_dir = '/hdd/HENRI/goosynth/test/'\n",
    "test_pickle_path = '/hdd/HENRI/goosynth/picklefiles/testpickle120.pickle'\n",
    "\n",
    "train_set = GooDataset(images_dir, pickle_path, 'train')\n",
    "train_data_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "\n",
    "val_set = GooDataset(test_images_dir, test_pickle_path, 'test')\n",
    "test_data_loader = torch.utils.data.DataLoader(val_set, batch_size=testbatchsize, num_workers=workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def test_random(test_data_loader, logger):\n",
    "    \n",
    "    total_loss = []\n",
    "    total_error = []\n",
    "    info_list = []\n",
    "    heatmaps = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_data_loader, total=len(test_data_loader)):\n",
    "                     \n",
    "            image, face_image, gaze_field, eye_position, gt_position, gt_heatmap = \\\n",
    "                data['image'], data['face_image'], data['gaze_field'], data['eye_position'], data['gt_position'], data['gt_heatmap']\n",
    "            image, face_image, gaze_field, eye_position, gt_position, gt_heatmap = \\\n",
    "                map(lambda x: Variable(x.cuda()), [image, face_image, gaze_field, eye_position, gt_position, gt_heatmap])\n",
    "\n",
    "            batch_size = image.shape[0]\n",
    "            \n",
    "            #direction, predict_heatmap = net([image, face_image, gaze_field, eye_position])\n",
    "            #predict_heatmap = np.zeros(shape=(batch_size, 56*56))\n",
    "            #ridx = np.random.randint(low=0, high=56*56, size=(batch_size,))\n",
    "            #predict_heatmap[:, ridx] = 1.0\n",
    "            #predict_heatmap = predict_heatmap.reshape((-1, 56,56))\n",
    "            \n",
    "            predict_heatmap = np.random.normal(size=(batch_size, 56,56))\n",
    "            \n",
    "            final_output = predict_heatmap\n",
    "            target = gt_position.cpu().data.numpy()\n",
    "            eye_position = eye_position.cpu().data.numpy()\n",
    "            predict_heatmap = predict_heatmap\n",
    "\n",
    "            for f_point, gt_point, eye_point, heatmap in \\\n",
    "                zip(final_output, target, eye_position, predict_heatmap):\n",
    "                f_point = f_point.reshape([224 // 4, 224 // 4])\n",
    "                heatmaps.append(f_point)\n",
    "\n",
    "                h_index, w_index = np.unravel_index(f_point.argmax(), f_point.shape)\n",
    "                f_point = np.array([w_index / 56., h_index / 56.])\n",
    "\n",
    "                f_error = f_point - gt_point\n",
    "                f_dist = np.sqrt(f_error[0] ** 2 + f_error[1] ** 2)\n",
    "\n",
    "                # angle\n",
    "                f_direction = f_point - eye_point\n",
    "                gt_direction = gt_point - eye_point\n",
    "\n",
    "                norm_f = (f_direction[0] **2 + f_direction[1] ** 2 ) ** 0.5\n",
    "                norm_gt = (gt_direction[0] **2 + gt_direction[1] ** 2 ) ** 0.5\n",
    "\n",
    "                f_cos_sim = (f_direction[0]*gt_direction[0] + f_direction[1]*gt_direction[1]) / \\\n",
    "                            (norm_gt * norm_f + 1e-6)\n",
    "                f_cos_sim = np.maximum(np.minimum(f_cos_sim, 1.0), -1.0)\n",
    "                f_angle = np.arccos(f_cos_sim) * 180 / np.pi\n",
    "\n",
    "                #AUC\n",
    "                heatmap = np.squeeze(heatmap)\n",
    "                heatmap = cv2.resize(heatmap, (5, 5))\n",
    "                gt_heatmap = np.zeros((5, 5))\n",
    "                x, y = list(map(int, gt_point * 5))\n",
    "                gt_heatmap[y, x] = 1.0\n",
    "                \n",
    "                score = roc_auc_score(gt_heatmap.reshape([-1]).astype(np.int32), heatmap.reshape([-1]))\n",
    "\n",
    "                total_error.append([score, f_dist, f_angle])\n",
    "                info_list.append(list(f_point))\n",
    "    info_list = np.array(info_list)\n",
    "\n",
    "    logger.info('average error: %s'%str(np.mean(np.array(total_error), axis=0)))\n",
    "\n",
    "    return np.mean(np.array(total_error), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [07:54<00:00,  1.26it/s]\n",
      "average error: [ 0.49728516  0.45361068 76.97099224]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.49728516,  0.45361068, 76.97099224])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_random(test_data_loader, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
